# coding: utf-8
import os
import time
import sys
import yaml
import math
import numpy as np
import pandas as pd
from src.util import FileLevelOLEDataset, OLEDataset, ExeDataset,write_pred
from src.model import FileLevelModel
from torch.utils.data import DataLoader
import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable

#dataset = OLEDataset("./train_dataset.csv")
dataset = FileLevelOLEDataset("./oledata/test", "./file_labels.csv")

first_data = dataset[0]
features, labels = first_data
input_size = len(features)
print(f"Input size: {input_size}")
print("First feature and label")
print(features, labels)
print()

batch_size = 1 
dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=2)
num_epochs = 100
total_samples = len(dataset)
n_iterations = math.ceil(total_samples/batch_size)
print(total_samples, n_iterations)

model = FileLevelModel(1018, 500)
# 2. Construct loss and optimizer
## Loss
learning_rate = .001
loss = nn.BCEWithLogitsLoss()
## Optimizer
optimizer = optim.Adam([{'params':model.parameters()}],lr=learning_rate)
for epoch in range(num_epochs):
    for i, (streams, labels) in enumerate(dataloader): 
        #print(streams, labels)
        # 1. Forward = compute prediction, compute loss
        ## compute prediction
        # x = torch.IntTensor(batch_data[0])
        #print(f"Stream: {streams}")
        y_predicted = model(streams) # @Current, there are some problem with the compatibility of the data with the model
        #print(f"Y predicted: {y_predicted}, Labels: {labels}")
        # compute loss
        l = loss(y_predicted, labels)
        # 2. Backward = compute gradients
        l.backward()
        # # # 3. Update weights
        optimizer.step()
        # # ## Zero the gradient after updating
        optimizer.zero_grad()
        # # ## Print stuff
        if (i + 1) % (batch_size + 1) == 0: 
            print(f"epoch: {epoch + 1}/{num_epochs}, step {i + 1}/{n_iterations}, streams {streams.shape}, loss: {l.item()}")
        # # # w, b = model.parameters()
        # print(f"Step {total_step + 1}: loss={l.item()}")
        # total_step += 1
